{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Install Java\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "\n",
        "# Install PySpark\n",
        "!pip install pyspark\n",
        "\n",
        "# Install GraphFrames\n",
        "!pip install graphframes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NTxJyOAuS6WX",
        "outputId": "ad23cb61-e061-45c4-de28-63d01ce06363"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.3.tar.gz (317.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.3/317.3 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.3-py2.py3-none-any.whl size=317840625 sha256=e5b453538524200182479f8c51771ee9d8f9f133131d268ed8a0141d39b0b9e4\n",
            "  Stored in directory: /root/.cache/pip/wheels/1b/3a/92/28b93e2fbfdbb07509ca4d6f50c5e407f48dce4ddbda69a4ab\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.3\n",
            "Collecting graphframes\n",
            "  Downloading graphframes-0.6-py2.py3-none-any.whl.metadata (934 bytes)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from graphframes) (1.26.4)\n",
            "Collecting nose (from graphframes)\n",
            "  Downloading nose-1.3.7-py3-none-any.whl.metadata (1.7 kB)\n",
            "Downloading graphframes-0.6-py2.py3-none-any.whl (18 kB)\n",
            "Downloading nose-1.3.7-py3-none-any.whl (154 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.7/154.7 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nose, graphframes\n",
            "Successfully installed graphframes-0.6 nose-1.3.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xOHMOkQKSrYT",
        "outputId": "9f42d589-a324-4858-cc98-fdf1afe74249"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+----------+--------------+-----------------+-------------------+----------------------+---------+------+---------+\n",
            "|airline|airline ID|source_airport|source_airport_id|destination_airport|destination_airport_id|codeshare| stops|equipment|\n",
            "+-------+----------+--------------+-----------------+-------------------+----------------------+---------+------+---------+\n",
            "|     5T|      1623|           YRT|              132|                YEK|                    50|     NULL|     1|      ATR|\n",
            "|     AC|       330|           ABJ|              253|                BRU|                   302|     NULL|     1|      333|\n",
            "|     AC|       330|           YVR|              156|                YBL|                    30|     NULL|     1|      BEH|\n",
            "|     CU|      1936|           FCO|             1555|                HAV|                  1909|     NULL|     1|      767|\n",
            "|     FL|      1316|           HOU|             3566|                SAT|                  3621|     NULL|     1|      735|\n",
            "+-------+----------+--------------+-----------------+-------------------+----------------------+---------+------+---------+\n",
            "only showing top 5 rows\n",
            "\n",
            "Vertices DataFrame:\n",
            "+---+\n",
            "| id|\n",
            "+---+\n",
            "|NBC|\n",
            "|HYL|\n",
            "|YUL|\n",
            "|PMI|\n",
            "|SCW|\n",
            "|LEB|\n",
            "|BGM|\n",
            "|KGL|\n",
            "|CLQ|\n",
            "|NWI|\n",
            "|KLR|\n",
            "|LEN|\n",
            "|GIS|\n",
            "|CCK|\n",
            "|FAV|\n",
            "|PKE|\n",
            "|HEL|\n",
            "|YZS|\n",
            "|MDL|\n",
            "|ELI|\n",
            "+---+\n",
            "only showing top 20 rows\n",
            "\n",
            "Edges DataFrame:\n",
            "+---+---+\n",
            "|src|dst|\n",
            "+---+---+\n",
            "|YRT|YEK|\n",
            "|ABJ|BRU|\n",
            "|YVR|YBL|\n",
            "|FCO|HAV|\n",
            "|HOU|SAT|\n",
            "|MCO|HOU|\n",
            "|MCO|ORF|\n",
            "|ARN|GEV|\n",
            "|BOS|MCO|\n",
            "|MCO|BOS|\n",
            "|MCO|CAK|\n",
            "|AER|KZN|\n",
            "|ASF|KZN|\n",
            "|ASF|MRV|\n",
            "|CEK|KZN|\n",
            "|CEK|OVB|\n",
            "|DME|KZN|\n",
            "|DME|NBC|\n",
            "|DME|TGK|\n",
            "|DME|UUA|\n",
            "+---+---+\n",
            "only showing top 20 rows\n",
            "\n",
            "Grouped Edges DataFrame:\n",
            "+---+---+-----+------------+-----------------+\n",
            "|src|dst|count|source_color|destination_color|\n",
            "+---+---+-----+------------+-----------------+\n",
            "|ORD|ATL|   20|     #3358FF|          #FF3F33|\n",
            "|ATL|ORD|   19|     #3358FF|          #FF3F33|\n",
            "|HKT|BKK|   13|     #3358FF|          #FF3F33|\n",
            "|ORD|MSY|   13|     #3358FF|          #FF3F33|\n",
            "|JFK|LHR|   12|     #3358FF|          #FF3F33|\n",
            "|CAN|HGH|   12|     #3358FF|          #FF3F33|\n",
            "|MIA|ATL|   12|     #3358FF|          #FF3F33|\n",
            "|LHR|JFK|   12|     #3358FF|          #FF3F33|\n",
            "|ATL|MIA|   12|     #3358FF|          #FF3F33|\n",
            "|HKG|BKK|   12|     #3358FF|          #FF3F33|\n",
            "|DOH|BAH|   12|     #3358FF|          #FF3F33|\n",
            "|AUH|MCT|   12|     #3358FF|          #FF3F33|\n",
            "|BKK|HKG|   12|     #3358FF|          #FF3F33|\n",
            "|KGL|EBB|   11|     #3358FF|          #FF3F33|\n",
            "|MSY|JFK|   11|     #3358FF|          #FF3F33|\n",
            "|JFK|CDG|   11|     #3358FF|          #FF3F33|\n",
            "|ATL|DEN|   11|     #3358FF|          #FF3F33|\n",
            "|JFK|MSY|   11|     #3358FF|          #FF3F33|\n",
            "|LAX|LHR|   11|     #3358FF|          #FF3F33|\n",
            "|LHR|LAX|   11|     #3358FF|          #FF3F33|\n",
            "+---+---+-----+------------+-----------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import desc, col, lit\n",
        "\n",
        "# Create SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Airline Routes Analysis\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Read data from airline_routes.csv\n",
        "df = spark.read.csv('/content/airline_routes.csv', header=True, inferSchema=True)\n",
        "\n",
        "# Show the DataFrame\n",
        "df.show(5)\n",
        "\n",
        "# Create Vertices DataFrame using source_airport as the id column\n",
        "vertices = df.select(\"source_airport\").withColumnRenamed(\"source_airport\", \"id\").distinct()\n",
        "\n",
        "# Create Edge DataFrame using source_airport as src and destination_airport as dst\n",
        "edges = df.select(\"source_airport\", \"destination_airport\") \\\n",
        "    .withColumnRenamed(\"source_airport\", \"src\") \\\n",
        "    .withColumnRenamed(\"destination_airport\", \"dst\")\n",
        "\n",
        "# Show Vertices DataFrame\n",
        "print(\"Vertices DataFrame:\")\n",
        "vertices.show()\n",
        "\n",
        "# Show Edge DataFrame\n",
        "print(\"Edges DataFrame:\")\n",
        "edges.show()\n",
        "\n",
        "# Group the edges based on the src and dst where the count must be more than 5\n",
        "edges_grouped = edges.groupBy(\"src\", \"dst\") \\\n",
        "    .count() \\\n",
        "    .filter(\"count > 5\") \\\n",
        "    .orderBy(desc(\"count\")) \\\n",
        "    .withColumn(\"source_color\", lit(\"#3358FF\")) \\\n",
        "    .withColumn(\"destination_color\", lit(\"#FF3F33\"))\n",
        "\n",
        "# Show grouped data\n",
        "print(\"Grouped Edges DataFrame:\")\n",
        "edges_grouped.show()\n",
        "\n",
        "# Write data into new_data.csv using the overwrite mode and set the header to True\n",
        "edges_grouped.write.mode(\"overwrite\").option(\"header\", True).csv('/content/new_data.csv')\n",
        "\n",
        "# Stop the Spark session\n",
        "spark.stop()\n"
      ]
    }
  ]
}