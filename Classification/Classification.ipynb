{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark\n",
        "!pip install seaborn matplotlib\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h5Q0-Nuz6g07",
        "outputId": "bc86c952-29fd-47ee-cf9f-3d6e702a05a8"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.2.tar.gz (317.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.3/317.3 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.2-py2.py3-none-any.whl size=317812365 sha256=95f2bfb7dece468ff6dfd6781367daec2f7ace6d64dbf42f0f79244329150796\n",
            "  Stored in directory: /root/.cache/pip/wheels/34/34/bd/03944534c44b677cd5859f248090daa9fb27b3c8f8e5f49574\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.2\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.10/dist-packages (0.13.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n",
            "Requirement already satisfied: numpy!=1.24.0,>=1.20 in /usr/local/lib/python3.10/dist-packages (from seaborn) (1.26.4)\n",
            "Requirement already satisfied: pandas>=1.2 in /usr/local/lib/python3.10/dist-packages (from seaborn) (2.1.4)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (24.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2->seaborn) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2->seaborn) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Exp7tEK15X5T",
        "outputId": "1f8b8f01-e7e8-4d03-bf51-47656f82e258"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------+----------+\n",
            "|status_type_ind|prediction|\n",
            "+---------------+----------+\n",
            "|            0.0|       0.0|\n",
            "|            0.0|       0.0|\n",
            "|            0.0|       0.0|\n",
            "|            0.0|       0.0|\n",
            "|            0.0|       0.0|\n",
            "+---------------+----------+\n",
            "only showing top 5 rows\n",
            "\n",
            "Accuracy: 0.5778748180494906\n",
            "Weighted Precision: 0.35540867972989576\n",
            "Weighted Recall: 0.5778748180494906\n",
            "F1 Score: 0.44012719955040336\n"
          ]
        }
      ],
      "source": [
        "# Step 1: Import necessary libraries\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "from pyspark.ml import Pipeline\n",
        "\n",
        "# Step 2: Create a SparkSession\n",
        "spark = SparkSession.builder.appName(\"LogisticRegressionFBLive\").getOrCreate()\n",
        "\n",
        "# Step 3: Load data into DataFrame (fb_live_thailand.csv)\n",
        "data = spark.read.csv(\"fb_live_thailand.csv\", header=True, inferSchema=True)\n",
        "\n",
        "# Step 4: Use StringIndexer for 'status_type' and 'status_published' to create indexes\n",
        "indexer_status_type = StringIndexer(inputCol=\"status_type\", outputCol=\"status_type_ind\")\n",
        "indexer_status_published = StringIndexer(inputCol=\"status_published\", outputCol=\"status_published_ind\")\n",
        "\n",
        "# Fit and transform the data with the indexers\n",
        "indexed_data = indexer_status_type.fit(data).transform(data)\n",
        "indexed_data = indexer_status_published.fit(indexed_data).transform(indexed_data)\n",
        "\n",
        "# Step 5: Use VectorAssembler to create a feature vector of 'status_type_ind' and 'status_published_ind'\n",
        "assembler = VectorAssembler(inputCols=[\"status_type_ind\", \"status_published_ind\"], outputCol=\"features\")\n",
        "\n",
        "# Step 6: Create Logistic Regression where 'status_type_ind' is the label\n",
        "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"status_type_ind\", maxIter=10, regParam=0.3, elasticNetParam=0.8)\n",
        "\n",
        "# Step 7: Create a pipeline with VectorAssembler and LogisticRegression\n",
        "pipeline = Pipeline(stages=[assembler, lr])\n",
        "\n",
        "# Step 8: Split the data into train and test datasets\n",
        "train_data, test_data = indexed_data.randomSplit([0.8, 0.2], seed=1234)\n",
        "\n",
        "# Step 9: Fit the train data into the pipeline to create the model\n",
        "lr_model = pipeline.fit(train_data)\n",
        "\n",
        "# Step 10: Use the model to transform the test data to get predictions\n",
        "predictions = lr_model.transform(test_data)\n",
        "\n",
        "# Step 11: Show 5 rows of the predictions DataFrame\n",
        "predictions.select(\"status_type_ind\", \"prediction\").show(5)\n",
        "\n",
        "# Step 12: Use MulticlassClassificationEvaluator to evaluate the model\n",
        "evaluator = MulticlassClassificationEvaluator(labelCol=\"status_type_ind\", predictionCol=\"prediction\")\n",
        "\n",
        "# Evaluate Accuracy\n",
        "accuracy = evaluator.setMetricName(\"accuracy\").evaluate(predictions)\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "\n",
        "# Evaluate Precision\n",
        "precision = evaluator.setMetricName(\"weightedPrecision\").evaluate(predictions)\n",
        "print(f\"Weighted Precision: {precision}\")\n",
        "\n",
        "# Evaluate Recall\n",
        "recall = evaluator.setMetricName(\"weightedRecall\").evaluate(predictions)\n",
        "print(f\"Weighted Recall: {recall}\")\n",
        "\n",
        "# Evaluate F1 Score\n",
        "f1 = evaluator.setMetricName(\"f1\").evaluate(predictions)\n",
        "print(f\"F1 Score: {f1}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Decision tree classification"
      ],
      "metadata": {
        "id": "whvSg1IT8NeL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.feature import StringIndexer, VectorAssembler, OneHotEncoder\n",
        "from pyspark.ml.classification import DecisionTreeClassifier\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "from pyspark.ml import Pipeline\n",
        "\n",
        "# Create a SparkSession\n",
        "spark = SparkSession.builder.appName(\"DecisionTreeClassificationExample\").getOrCreate()\n",
        "\n",
        "# Load data file into DataFrame (assuming the file is named 'fb_live_thailand.csv' and located in the current directory)\n",
        "df = spark.read.csv(\"fb_live_thailand.csv\", header=True, inferSchema=True)\n",
        "\n",
        "# Check the initial DataFrame schema\n",
        "df.printSchema()\n",
        "\n",
        "# Use StringIndexer to index categorical columns\n",
        "status_type_indexer = StringIndexer(inputCol=\"status_type\", outputCol=\"status_type_ind\")\n",
        "status_published_indexer = StringIndexer(inputCol=\"status_published\", outputCol=\"status_published_ind\")\n",
        "\n",
        "# Transform the data to add indexed columns\n",
        "df_indexed = status_type_indexer.fit(df).transform(df)\n",
        "df_indexed = status_published_indexer.fit(df_indexed).transform(df_indexed)\n",
        "\n",
        "# Use OneHotEncoder to encode the indexed columns\n",
        "encoder = OneHotEncoder(inputCols=[\"status_type_ind\", \"status_published_ind\"], outputCols=[\"status_type_vec\", \"status_published_vec\"])\n",
        "\n",
        "# Use VectorAssembler to create a features vector\n",
        "assembler = VectorAssembler(inputCols=[\"status_type_vec\", \"status_published_vec\"], outputCol=\"features\")\n",
        "\n",
        "# Create a Decision Tree Classifier\n",
        "dt = DecisionTreeClassifier(labelCol=\"status_type_ind\", featuresCol=\"features\")\n",
        "\n",
        "# Create a pipeline\n",
        "pipeline = Pipeline(stages=[encoder, assembler, dt])\n",
        "\n",
        "# Create train and test datasets\n",
        "train_data, test_data = df_indexed.randomSplit([0.7, 0.3], seed=42)\n",
        "\n",
        "# Fit the pipeline model on the training data\n",
        "pipeline_model = pipeline.fit(train_data)\n",
        "\n",
        "# Use the pipeline model to make predictions\n",
        "predictions = pipeline_model.transform(test_data)\n",
        "\n",
        "# Show 5 rows of the predictions DataFrame\n",
        "predictions.select(\"status_type_ind\", \"prediction\").show(5)\n",
        "\n",
        "# Create a MulticlassClassificationEvaluator\n",
        "evaluator = MulticlassClassificationEvaluator(labelCol=\"status_type_ind\", predictionCol=\"prediction\")\n",
        "\n",
        "# Evaluate metrics\n",
        "accuracy = evaluator.evaluate(predictions, {evaluator.metricName: \"accuracy\"})\n",
        "precision = evaluator.evaluate(predictions, {evaluator.metricName: \"weightedPrecision\"})\n",
        "recall = evaluator.evaluate(predictions, {evaluator.metricName: \"weightedRecall\"})\n",
        "f1 = evaluator.evaluate(predictions, {evaluator.metricName: \"f1\"})\n",
        "\n",
        "# Show metrics\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "print(f\"Precision: {precision}\")\n",
        "print(f\"Recall: {recall}\")\n",
        "print(f\"F1 Score: {f1}\")\n",
        "\n",
        "# Calculate and show Test Error\n",
        "test_error = 1.0 - accuracy\n",
        "print(f\"Test Error: {test_error}\")\n",
        "\n",
        "# Stop the SparkSession\n",
        "spark.stop()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X04K3krS-w98",
        "outputId": "08631664-06ce-4316-c0af-9ad23fcb9609"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- status_id: string (nullable = true)\n",
            " |-- status_type: string (nullable = true)\n",
            " |-- status_published: string (nullable = true)\n",
            " |-- num_reactions: integer (nullable = true)\n",
            " |-- num_comments: integer (nullable = true)\n",
            " |-- num_shares: integer (nullable = true)\n",
            " |-- num_likes: integer (nullable = true)\n",
            " |-- num_loves: integer (nullable = true)\n",
            " |-- num_wows: integer (nullable = true)\n",
            " |-- num_hahas: integer (nullable = true)\n",
            " |-- num_sads: integer (nullable = true)\n",
            " |-- num_angrys: integer (nullable = true)\n",
            "\n",
            "+---------------+----------+\n",
            "|status_type_ind|prediction|\n",
            "+---------------+----------+\n",
            "|            0.0|       0.0|\n",
            "|            0.0|       0.0|\n",
            "|            0.0|       0.0|\n",
            "|            0.0|       0.0|\n",
            "|            0.0|       0.0|\n",
            "+---------------+----------+\n",
            "only showing top 5 rows\n",
            "\n",
            "Accuracy: 1.0\n",
            "Precision: 1.0\n",
            "Recall: 1.0\n",
            "F1 Score: 1.0\n",
            "Test Error: 0.0\n"
          ]
        }
      ]
    }
  ]
}